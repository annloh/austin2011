---
title: "Manuscript Information Extraction"
author: "Anna Lohmann"
date: "5/7/2020"
output: html_document
---

25% of the sample was exposed to the treatment
probability of the outcome would be 0.29 if everyone were not exposed
ATT *risk difference due to treatment
0, -0.02, -0.05, -0.10, -0.15
```{r}
att <- c(0, -0.02, -0.05, -0.10, -0.15)
```

i.e. absolute reduction in the probability of the outcome was
0, 0.02, 0.05, 0.10 and 0.15

Non-null risk difference  is equivlent to NNTs
50, 20, 10 and 7

## Data generating mechanism

- 10 independent covariates
- independent standard normal distributions
- 10 000 subjects


## First scenario (independent normal covariates)
```{r}
# Scenario 1 (baseline covariates independant standard normal)

x_1 <- rnorm(n = 10000, mean = 0, sd =1)
```


## Second scenario (correlated normal covariates)
10 covariates from multivariate normal distribution
mean =  0 and sd = 1, correlation between pairs of random
variables was 0.25

```{r}
#Scenario 2 

# Correlation matrice

cor_mat2 <- matrix(rep(0.25, 100), nrow = 10)
diag(cor_mat2) <- 1

```

## Third scenario (first mixed covariates scenario)
First five covariates independent Bernoullli random variables with p = 0.5

last five covariates independent standard normal
```{r}
x_1 <- rbinom(n = 10000, size = 1, prob = 0.5)
x_2 <- 

x_6 <- rnorm(n = 10000, mean = 0, sd =1)
```


## Fourth scenario (second mixed covariates scenario)

First nine covariates independent Bernoullli random variables with p = 0.5
Tenth covariate standard normal

## Fifth scenario (binary covariates scenario)
all 10 covariates independent Bernoulli with parameter 0.5


Probability of treatment was related to the 10 baseline covariates in teh following way:
Equation 1:
logit($p_{i,treat}$}) = $\alpha_{0,treat} + \alpha_LX_{1,i} + \alpha_LX_{2,i} + \alpha_LX_{3,i} + 
\alpha_MX_{4,i} + \alpha_MX_{5,i} + \alpha_MX_{5,i} + \alpha_HX_{7,i} + 
\alpha_HX_{8,i}  + \alpha_HX_{9,i} + \alpha_VHX_{10,i}$

Regression coefficients were set as following
```{r}
alpha_l_bin <- log(1.1)
alpha_m_bin <- log(1.25)
alpha_h_bin <- log(1.5)
alpha_vh_bin <- log(2)
```
Intended to reflect low, medium, high and very high effect sizes.

Intercept $\alpha_{0,outcome}$ was fixed to 
```{r}
alpha_0_outcome <- log(0.29/0.71)
```
so that the probability ofthe event occuring in the population if all subjects were untreated was approximately 0.29


treatment indicator
$Z_i$ was generated for each subject from a Bernoulli distribution with subject-specific
probability = $p_{i,treat}$


```{r}
p_treat <- 
```

##Binary outcome

```{r}

alpha <- c(alpha_l, alpha_l, alpha_l, 
                    alpha_m, alpha_m, alpha_m,
                    alpha_h, alpha_h, alpha_h,
                    alpha_vh)

alpha <- c(log(0.29/0.71), rep(log(1.1), 3), rep(log(1.25), 3), rep(log(1.5), 3), log(2))

x <- cbind(1, x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10)

y_lin <- alpha_0_treat + t(alpha) %*% t(x)
          
prob_treat <- 1/(1 + exp(-y_lin))
# Compute treatment indicator

treatment_indicator <- 


```

Logistic regression model for the generation of binary outcomes


Subjects with $Z_i = 1$ denote treated subjects in whom the ATT is defined


Outcome probability was related to teh covariated in the following way

logit($p_{i,outcome}$}) = $$\alpha_{0,outcome} + \betaZ_i + \alpha_LX_{1,i} + 
\alpha_LX_{2,i} + \alpha_LX_{3,i} + 
\alpha_MX_{4,i} + \alpha_MX_{5,i} + \alpha_MX_{5,i} + \alpha_HX_{7,i} + 
\alpha_HX_{8,i}  + \alpha_HX_{9,i} + \alpha_VHX_{10,i}$$

Subject specific were generated from a Bernoulli destribution with probability 
$p_{i, outcome}

```{r}
beta_bin <- c(0, 0.9077272, 0.7836084, 0.6086645, 0.4658031)

outcome_bin <- alpha_0_outcome + beta_bin * treatment_indicator + t(alpha) %*% t(x)
```
Reference [18] should provide information how these values were determined
$\beta$ depends on the distribution of baseline covariates in the population
and on the population of treated subjects.


## Generation of continous outcome
$$Y_i = \alpha_{0,outcome} + \betaZ_i + \alpha_LX_{1,i} + 
\alpha_LX_{2,i} + \alpha_LX_{3,i} + 
\alpha_MX_{4,i} + \alpha_MX_{5,i} + \alpha_MX_{5,i} + \alpha_HX_{7,i} + 
\alpha_HX_{8,i}  + \alpha_HX_{9,i} + \alpha_VHX_{10,i}$$

where $\epsilon ~ N(0,\sigma^2)$ 
with $\sigma^2 = 127.6056$ which corresponds to a model  $R^2 = 0.13$
implying that the 10 measured baseline covariates explain 13% of the variation 
in the outcome.
```{r}
#Definiton of regression weights for outcome
alpha_l_cont <- 1.1 # corresponds to a low effect size
alpha_m_cont <- 1.25 #corresponds to a medium effect size
alpha_h_cont <- 1.5 #corresponds to a high effect size
alpha_vh_cont <- 2 # correspons to a very high effect size

alpha_vec <- c(alpha_l_cont, alpha_l_cont, alpha_l_cont, alpha_m_cont, alpha_m_cont, alpha_m_cont, alpha_h_cont, alpha_h_cont, alpha_h_cont, alpha_vh_cont)

beta_cont <- c(0, 1.1, 1.25, 1.5, 2)

alpha_0_outcome_cont <- 0

outcome_cont <- alpha_0_outcome + beta * treatment_indicator + 
```
Exposure increased the mean of the response variable Y by $\beta_{out}$.


1000 data sets per scenario

$\gamma$ ranged from 0.05 to 2.5 in increments of 0.05
hence 50 different propensity-score matched samples 
were formed from each randomly generated dataset

```{r}
gamma <- seq(from = 0.05, to = 2.5, by = 0.05)
```

```{r}

# here a function, ideally a vectorized one
compute_propensity_score <- function()

  
data %>% mutate(propensity_score = compute_propensity_score(covariates))
calipher_width <- gamma*sqrt((sigma_squared_treated + sigma_squared_untreated)/2)  
    
assign_group <- function(gamma, propensity_score)  
```



subjects were matched on the logit of the propensity score using a calipher of width equal to
$\gamma \sqrt(\sigma_1^2 + \sigma_2^2)/2$
where $\sigma_i^2$ is the variance of the logit of the propensity score in the ith group

On the propensity-score matched sample the absolute risk reduction was estimated 
as the difference between the proportion of treated subjects experiencing the
outcome and the proportion of untreated subjects experiencing the outcome in the 
matched sample

statistical significance of the risk difference was tested using Mc Nemar's test
for correlated binomial proportions

```{r}
prop_outcome <- data %>% group_by(treated) %>% summarize(mean = mean(outcome))

mcnemar.test(prop_outcome)

confidence_interval
```

Difference in the probability of the event between treated and untreated
```{r}
b <- pairs %>% filter(outcome_treated == 1, outcome_untreated == 0) %>% summarize(n = n())
c <- pairs %>% filter(outcome_treated == 0, outcome_untreated == 1) %>% summarize(n = n())
d <- pairs %>% filter(outcome_treated == 0, outcome_untreated == 0) %>% summarize(n = n())

diff_prob_event <- (b-c)/n_row(pairs)

var_diff_prob_event <- ((b + c) - (c-b^2)/ n_row(pairs))/n_row(pairs)^2

# crude unadjusted risk difference
mean_estimated_risk_difference <- 

```

